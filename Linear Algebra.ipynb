{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A. MATHEMATICAL PRELIMINARIES\n",
    "\n",
    "# A.1. Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector notation and definitions\n",
    "\n",
    "An $n$-dimensional *vector* $\\V{x}$ is a tuple of real numbers\n",
    "$\\V{x}=(x_1,\\ldots,x_n) \\in \\Reals^n$. In later portions of this course,\n",
    "we will usually drop the boldface.\n",
    "\n",
    "Vectors can be added and subtracted component-wise, can be multipled by\n",
    "elements of $\\Reals$, and can be divided by elements of\n",
    "$\\Reals \\setminus \\{0\\}$. There is a zero vector $\\V{0}$ with all\n",
    "elements equal to zero. Each element has a component-wise negation,\n",
    "$-\\V{x}$. We will also on occasion refer to the standard *basis vectors*\n",
    "$\\V{e_1},\\ldots,\\V{e_n}$, where $\\V{e_i}$ has all elements equal to 0\n",
    "except for the $i$'th element, which is equal to 1. More commonly, we\n",
    "refer to $\\V{e_1}$ as the x-axis, $\\V{e_2}$ as the y-axis, and so on.\n",
    "\n",
    "> **Dot product**. The dot product is a function that takes two vectors\n",
    "> $\\V{x}=(x_1,\\ldots,x_n)$ and $\\V{y}=(y_1,\\ldots,y_n)$ and returns a real\n",
    "> number, and is given by the expression\n",
    "> $$\\V{x} \\cdot \\V{y} = \\sum_{i=1}^n x_i y_i.$$\n",
    "\n",
    "For example, $\\V{x} \\cdot \\V{e_i} = x_i$.\n",
    "\n",
    "> **Orthogonal vectors**. Two vectors whose dot product is identically zero are called\n",
    "> *orthogonal*.\n",
    "\n",
    "For example, $\\V{e_i} \\cdot \\V{e_j} = 0$ for all $i\\neq j$, and so $\\V{e_i}$ and $\\V{e_i}$ are orthogonal.\n",
    "\n",
    "> **Norms**. A norm describes some notion of vector magnitude. The standard Euclidean\n",
    "> norm $\\|\\cdot\\|$ is defined as $\\|\\V{x}\\|=\\sqrt{\\V{x} \\cdot \\V{x}}$.\n",
    "\n",
    "Some identities are $\\|\\V{0}\\|=0$, $\\|\\V{x}\\| > 0$ if\n",
    "$\\V{x} \\neq \\V{0}$, and $\\|c\\V{x}\\| = |c|\\,\\|\\V{x}\\|$ for all real values $c$.\n",
    "\n",
    "> **Unit vectors**. A unit vector is a vector with unit norm: $\\|\\V{x}\\|=1$.\n",
    "\n",
    "> **Euclidean distance**. The euclidean distance between two vectors is given by the norm of the\n",
    "> difference between the vectors: $d(\\V{x},\\V{y}) = \\|\\V{x}-\\V{y}\\|$.\n",
    "\n",
    "It satisfies all the criteria of a metric, and hence vector spaces are metric spaces.\n",
    "\n",
    "> **Cosine angle formula**. The cosine of the angle between two unit vectors is equal to their dot\n",
    "> product, i.e. $\\cos(\\theta) = \\V{a} \\cdot \\V{b}$.\n",
    "\n",
    "> **Linear Combinations**. $\\V{x}$ is a *linear combination* of a set of vectors\n",
    "> $\\{\\V{a_1},\\ldots,\\V{a_m}\\}$ if $$\\V{x} = \\sum_{i=1}^m u_i \\V{a_i}$$ for\n",
    "> some set of numbers $u_1,\\ldots,u_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "A matrix $A$ represents a linear transformation of an $n$-dimensional\n",
    "vector space to an $m$-dimensional one. It is given by an $m\\times n$\n",
    "array of real numbers. Usually matrices are denoted as uppercase letters\n",
    "(e.g., $A, B, C$), with the entry in the $i$'th row and $j$'th column\n",
    "denoted in the subscript $\\cdot_{i,j}$, or when it is unambiguous,\n",
    "$\\cdot_{ij}$ (e.g., $A_{1,2}, A_{1p}$). $$A = \\left[ \\begin{array}{ccc}\n",
    "A_{1,1} & \\cdots & A_{1,n} \\\\\n",
    "\\vdots &   & \\vdots \\\\\n",
    "A_{m,n} & \\cdots & A_{m,n}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "### Matrix-Vector Product\n",
    "\n",
    "An $m\\times n$ matrix $A$ transforms vectors $\\V{x} =(x_1,\\ldots,x_n)$\n",
    "into $m$-dimensional vectors $\\V{y} = (y_1,\\ldots,y_m) = A\\V{x}$ as\n",
    "follows: $$\\begin{split}\n",
    "y_1 = \\sum_{j=1}^n A_{1j} x_j \\\\\n",
    "\\ldots \\\\\n",
    "y_m = \\sum_{j=1}^n A_{mj} x_j \\\\\n",
    "\\end{split}$$ Or, more concisely, $y_i = \\sum_{j=1}^n A_{ij} x_j$ for\n",
    "$i=1,\\ldots,m$. (Note that matrix-vector multiplication *is not\n",
    "symmetric*, so $\\V{x} A$ is an invalid operation.)\n",
    "\n",
    "#### Linearity of matrix-vector multiplication\n",
    "\n",
    "We can see that matrix-vector multiplication is linear, that is\n",
    "$A(a\\V{x}+b\\V{y}) = a A \\V{x} + b A \\V{y}$ for all $a$, $b$, $\\V{x}$,\n",
    "and $\\V{y}$. It is also linear in terms of component-wise addition and\n",
    "multiplication of matrices, as long as the matrices are of the same\n",
    "size. More precisely, if $A$ and $B$ are both $m\\times n$ matrices, then\n",
    "$(a A + b B)\\V{x} = a A\\V{x} + b B\\V{x}$ for all $a$, $b$, and $\\V{x}$.\n",
    "\n",
    "#### Identity matrix\n",
    "One special matrix that occurs frequently is the $n\\times n$ *identity\n",
    "matrix* $I_n$, which has 0's in all off-diagonal positions $I_{ij}$ with\n",
    "$i\\neq j$, and 1's in all diagonal positions $I_{ii}$. It is significant\n",
    "because $I_n \\V{x} = \\V{x}$ for all $\\V{x} \\in \\Reals^n$.\n",
    "\n",
    "### Matrix Product\n",
    "\n",
    "When two linear transformations are performed one after the other, the\n",
    "result is also a linear transformation. Suppose $A$ is $m\\times n$, $B$\n",
    "is $n \\times p$, and $\\V{x}$ is a $p$-dimensional vector, and consider\n",
    "the result of $A(B\\V{x})$ (that is, first multiplying by $B$ and then\n",
    "multiplying the result by $A$). We see that\n",
    "$$B\\V{x} = (\\sum_{j=1}^p B_{1j} x_j,\\ldots,\\sum_{j=1}^p B_{nj} x_j)$$\n",
    "and\n",
    "$$A \\V{y} = (\\sum_{k=1}^n A_{1k} y_k,\\ldots,\\sum_{k=1}^n A_{mk} y_k)$$\n",
    "So\n",
    "$$A (B \\V{x}) = \\left(\\sum_{k=1}^n A_{1k} (\\sum_{j=1}^p B_{kj} x_j),\\ldots,\\sum_{k=1}^n A_{mk} (\\sum_{j=1}^p B_{kj} x_j)\\right).$$\n",
    "Rearranging the summations, we see that\n",
    "$$A (B \\V{x}) = \\left(\\sum_{j=1}^p (\\sum_{k=1}^n A_{1k} B_{kj}) x_j),\\ldots,\\sum_{j=1}^p (\\sum_{k=1}^n A_{mk} B_{kj} x_j)\\right).$$\n",
    "In other words, we could have $A(B\\V{x}) = C \\V{x}$ if we were to form a\n",
    "matrix $C$ such that $$C_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}$$ This is\n",
    "exactly the definition of the *matrix product*, and we say $C=AB$. The\n",
    "entry $C_{ij}$ of can also be obtained taking the dot-product of the\n",
    "$i$'th column of $A$ and the $j$'th column of $B$.\n",
    "\n",
    "#### Matrix product is associative but not symmetric\n",
    "By the above derivation we can drop the parentheses\n",
    "$A(B\\V{x}) = (AB)\\V{x}$. So, matrix-vector and matrix-matrix\n",
    "multiplication are *associative*. Note again however that matrix-matrix\n",
    "multiplication *is not symmetric*, that is $AB \\neq BA$ in general.\n",
    "\n",
    "#### Column and row vectors\n",
    "Note that if we were to write an $n$-dimensional vector $\\V{x}$ stacked\n",
    "in a $n\\times 1$ matrix $x$ (denoted in lowercase), we can turn the\n",
    "matrix-vector $\\V{y}=A\\V{x}$ into the matrix product $y = A x$. Here, if\n",
    "$A$ is an $m\\times n$ matrix, then $y$ is an $m\\times 1$ matrix.\n",
    "$$\\left[\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_m \\end{array}\\right]\n",
    " = \\left[ \\begin{array}{ccc}\n",
    "A_{1,1} & \\cdots & A_{1,n} \\\\\n",
    "\\vdots &   & \\vdots \\\\\n",
    "A_{m,n} & \\cdots & A_{m,n}\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\end{array}\\right]$$ Hence, there is a one-to-one correspondence\n",
    "between vectors and matrices with one column. These matrices are called\n",
    "column vectors and will be our default notation for vectors throughout\n",
    "the rest of the course. We will occasionally also deal with row vectors,\n",
    "which are matrices with a single row.\n",
    "\n",
    "### Transpose\n",
    "\n",
    "The transpose $A^T$ of a matrix $A$ simply switches $A$'s rows and\n",
    "columns. $$(A^T)_{ij} = A_{ji}.$$ If $A$ is $m \\times n$, then $A^T$ is\n",
    "$n \\times m$.\n",
    "\n",
    "> **Symmetric matrix**. A square matrix $A$ is symmetric iff $A = A^T$.\n",
    "\n",
    "### Matrix Inverse\n",
    "\n",
    "An *inverse* $A^{-1}$ of an $n\\times n$ square matrix $A$ is a matrix\n",
    "that satisfies the following equation: $$A A^{-1}  = A^{-1} A = I_n$$\n",
    "where $I_n$ is the identity matrix. Not all square matrices have an\n",
    "inverse, in which case we say $A$ is not invertible (or singular).\n",
    "Invertible matrices are significant because the unique solution $x$ to\n",
    "the system of linear equations $Ax = b$, is simply $A^{-1} b$. This\n",
    "holds for any $b$. If the matrix is not invertible, then such an\n",
    "equation may or may not have a solution.\n",
    "\n",
    "> **Orthogonal matrix**. An orthogonal matrix is a square matrix that satisfies $A A^T = I_n$. In\n",
    "other words, its transpose is its inverse.\n",
    "\n",
    "### Matrix identities\n",
    "\n",
    "Identities involving the transpose:\n",
    "\n",
    "-   $(cA)^T = c A^T$ for any real value $c$.\n",
    "\n",
    "-   $(A+B)^T = A^T + B^T$.\n",
    "\n",
    "-   $(AB)^T = B^T A^T$.\n",
    "\n",
    "-   All $1\\times 1$ matrices are symmetric, the identity matrix is\n",
    "    symmetric, and all uniform scalings of a symmetric matrix are\n",
    "    symmetric.\n",
    "\n",
    "-   $A + A^T$ is symmetric.\n",
    "\n",
    "-   The dot product $\\V{x} \\cdot \\V{y}$ is equal to $x^T y$, with $x$\n",
    "    and $y$ denoting the column vector representations of $\\V{x}$ and\n",
    "    $\\V{y}$, respectively.\n",
    "\n",
    "-   $x^T A y = y^T A^T x$, with $x$ and $y$ column vectors.\n",
    "\n",
    "Identities involving the inverse:\n",
    "\n",
    "-   $I_n^{-1} = I_n$.\n",
    "\n",
    "-   $(cA)^{-1} = \\frac{1}{c} A^{-1}$ for any real value $c\\neq 0$.\n",
    "\n",
    "-   $(AB)^{-1} = B^{-1}A^{-1}$ if both $B$ and $A$ are invertible.\n",
    "\n",
    "-   If $A$ and $B$ are invertible, then\n",
    "    $(ABA^{-1})^{-1} = A B^{-1}A^{-1}$.\n",
    "\n",
    "### Matrix Pseudoinverse\n",
    "\n",
    "The pseudoinverse is a generalization of the inverse of an $m \\times n$\n",
    "matrix $A$ that is used when an inverse does not exist. It can also be\n",
    "used when a matrix is not square. The pseudoinverse $A^+$ is defined as\n",
    "an $n \\times m$ matrix that has the following properties:\n",
    "\n",
    "1.  $A A^+ A = A$\n",
    "\n",
    "2.  $A^+ A A^+ = A^+$\n",
    "\n",
    "3.  $(A A^+)^T = A A^+$\n",
    "\n",
    "4.  $(A^+ A)^T = A^+ A$\n",
    "\n",
    "It has the following properties:\n",
    "\n",
    "1.  If $A$ is invertible, then $A^+ = A^{-1}$.\n",
    "\n",
    "2.  If multiple solutions to $A x = b$ exist, then $x = A^+ b$ is the\n",
    "    solution that minimizes $\\| x \\|$.\n",
    "\n",
    "3.  If no solutions to $A x = b$ exist, then $x = A^+ b$ is the solution\n",
    "    that minimizes $\\| A x - b \\|$ (least squares solution).\n",
    "\n",
    "The pseudoinverse is usually available in most major linear algebra\n",
    "systems. It can be computed using the singular value decomposition\n",
    "(SVD), which is itself one of the most useful tools in scientific\n",
    "computing (see [Appendix B.2](MatrixComputations.ipynb#Singular-value-decomposition) ).\n",
    "\n",
    "### Positive definiteness\n",
    "\n",
    "An $n\\times n$ matrix $A$ is called *symmetric positive definite*\n",
    "(s.p.d.), or just positive definite, if it is symmetric ($A=A^T$) and\n",
    "satisfies the following condition:\n",
    "$$\\V{x}^T A \\V{x} > 0 \\text{ for all }\\V{x}\\in \\mathbb{R}^n.$$  For\n",
    "example, an identity matrix is s.p.d., as is any matrix $A = B^T B$ with\n",
    "$B$ a matrix with rank $n$. An s.p.d. matrix is invertible.\n",
    "\n",
    "A matrix is positive semi-definite (p.s.d.) if the strict positivity\n",
    "condition is replaced with a nonnegativity condition:\n",
    "$$\\V{x}^T A \\V{x} \\geq 0 \\text{ for all }\\V{x}\\in \\mathbb{R}^n.$$ Any\n",
    "s.p.d. matrix is also p.s.d., and any matrix $A=B^TB$ is also p.s.d.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": true,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
